{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 (KNN - Euclidean Distance)\n",
    "\n",
    "<img src=\"images/knn_ex1-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex1-2.png\" width=\"100%\">\n",
    "\n",
    "### Since the majority is Setosa, the predicted Species is Setosa\n",
    "\n",
    "Using elbow curve: Derive a plot between error rate and K denoting values in a defined range. Then choose the K value as having a minimum error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 (KNN - Hamming Distance)\n",
    "\n",
    "<img src=\"images/knn_ex2-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex2-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex2-3.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex2-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex2-5.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 (Weighted KNN algorithm Discrete-Valued)\n",
    "\n",
    "Given the dataset:\n",
    "\n",
    "| Sl. No | Height | Weight | Target |\n",
    "|--------|--------|--------|--------|\n",
    "| 1      | 150    | 50     | Medium |\n",
    "| 2      | 155    | 55     | Medium |\n",
    "| 3      | 160    | 60     | Large  |\n",
    "| 4      | 161    | 59     | Large  |\n",
    "| 5      | 158    | 65     | Large  |\n",
    "| 6      | 157    | 54     | ?      |\n",
    "\n",
    "Let's calculate the Euclidean distance between the new data point (157, 54) and each existing data point, then determine the 3 nearest neighbors.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\text{Euclidean Distance (1)} &= \\sqrt{(157 - 150)^2 + (54 - 50)^2} = \\sqrt{49 + 16} = \\sqrt{65} \\approx 8.06 \\\\\n",
    "\\text{Euclidean Distance (2)} &= \\sqrt{(157 - 155)^2 + (54 - 55)^2} = \\sqrt{4 + 1} = \\sqrt{5} \\approx 2.24 \\\\\n",
    "\\text{Euclidean Distance (3)} &= \\sqrt{(157 - 160)^2 + (54 - 60)^2} = \\sqrt{9 + 36} = \\sqrt{45} \\approx 6.71 \\\\\n",
    "\\text{Euclidean Distance (4)} &= \\sqrt{(157 - 161)^2 + (54 - 59)^2} = \\sqrt{16 + 25} = \\sqrt{41} \\approx 6.40 \\\\\n",
    "\\text{Euclidean Distance (5)} &= \\sqrt{(157 - 158)^2 + (54 - 65)^2} = \\sqrt{1 + 121} = \\sqrt{122} \\approx 11.05 \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "The 3 nearest neighbors are: (2), (3), and (4).\n",
    "\n",
    "Now, let's calculate the weights for each neighbor using the inverse of squared distance:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\text{Weight for (2)} &= \\frac{1}{2.24^2} \\approx 0.20 \\\\\n",
    "\\text{Weight for (3)} &= \\frac{1}{6.71^2} \\approx 0.02 \\\\\n",
    "\\text{Weight for (4)} &= \\frac{1}{6.40^2} \\approx 0.02 \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Now, let's perform the weighted voting:\n",
    "\n",
    "For Target \"Medium\":\n",
    "$ \\text{Weighted Votes for Medium} = 0.20 \\times 1 + 0.02 \\times 0 + 0.02 \\times 0 = 0.20 $\n",
    "\n",
    "For Target \"Large\":\n",
    "$\\text{Weighted Votes for Large} = 0.20 \\times 0 + 0.02 \\times 1 + 0.02 \\times 1 = 0.04 $\n",
    "\n",
    "Since the weighted votes for \"Medium\" ($0.20$) are higher than for \"Large\" ($0.04$), we predict that the new individual's target category is \"Medium\".\n",
    "\n",
    "Therefore, based on the weighted KNN algorithm, the predicted target category for the new individual with Height 157 and Weight 54 is \"Medium\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4 (Weighted KNN algorithm Real-Valued)\n",
    "\n",
    "Given the dataset:\n",
    "\n",
    "| Sl. No | Height | Weight | Target |\n",
    "|--------|--------|--------|--------|\n",
    "| 1      | 150    | 50     | 1.5    |\n",
    "| 2      | 155    | 55     | 1.2    |\n",
    "| 3      | 160    | 60     | 1.8    |\n",
    "| 4      | 161    | 59     | 2.1    |\n",
    "| 5      | 158    | 65     | 1.7    |\n",
    "| 6      | 157    | 54     | ?      |\n",
    "\n",
    "Let's calculate the Euclidean distance between the new data point (157, 54) and each existing data point, then determine the 3 nearest neighbors.\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\text{Euclidean Distance (1)} &= \\sqrt{(157 - 150)^2 + (54 - 50)^2} = \\sqrt{49 + 16} = \\sqrt{65} \\approx 8.06 \\\\\n",
    "\\text{Euclidean Distance (2)} &= \\sqrt{(157 - 155)^2 + (54 - 55)^2} = \\sqrt{4 + 1} = \\sqrt{5} \\approx 2.24 \\\\\n",
    "\\text{Euclidean Distance (3)} &= \\sqrt{(157 - 160)^2 + (54 - 60)^2} = \\sqrt{9 + 36} = \\sqrt{45} \\approx 6.71 \\\\\n",
    "\\text{Euclidean Distance (4)} &= \\sqrt{(157 - 161)^2 + (54 - 59)^2} = \\sqrt{16 + 25} = \\sqrt{41} \\approx 6.40 \\\\\n",
    "\\text{Euclidean Distance (5)} &= \\sqrt{(157 - 158)^2 + (54 - 65)^2} = \\sqrt{1 + 121} = \\sqrt{122} \\approx 11.05 \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "The 3 nearest neighbors are: (2), (3), and (4).\n",
    "\n",
    "Now, let's calculate the weights for each neighbor using the inverse of squared distance:\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\text{Weight for (2)} &= \\frac{1}{2.24^2} \\approx 0.20 \\\\\n",
    "\\text{Weight for (3)} &= \\frac{1}{6.71^2} \\approx 0.02 \\\\\n",
    "\\text{Weight for (4)} &= \\frac{1}{6.40^2} \\approx 0.02 \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Now, let's perform the weighted voting:\n",
    "\n",
    "$\n",
    "\\text{Predicted Target} = \\frac{(0.20 \\times 1.2) + (0.02 \\times 1.8) + (0.02 \\times 2.1)}{0.20 + 0.02 + 0.02} \n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Predicted Target} = \\frac{(0.24) + (0.036) + (0.042)}{0.24 + 0.036 + 0.042} \n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Predicted Target} \\approx \\frac{0.278}{0.278} \n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Predicted Target} \\approx 1.0 \n",
    "$\n",
    "\n",
    "Therefore, based on the weighted KNN algorithm, the predicted target category value for the new individual with Height 157 and Weight 54 is approximately 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5\n",
    "\n",
    "Based on the information given in the table below, find the customer type of xq=C4 using K- NN. In given example consider all the instances and use locally weighted K-NN algorithm with kernel K(d(xq,xi)) = 1/ d(xq,xi)2.\n",
    "Apply min-max normalization on income to obtain [0,1] range. Consider profession and Region as nominal. Consider :Locality as ordinal variable with ranking order of [Village, Small Town, Suburban, Metropolitan]. Give equal weight to each attribute.\n",
    "\n",
    "Here's the formatted table:\n",
    "\n",
    "| Customer | Income | Profession     | Region   | Locality    | Category |\n",
    "|----------|--------|----------------|----------|-------------|----------|\n",
    "| C0       | 60000  | Doctor         | Hindi    | Village     | L1       |\n",
    "| C1       | 70000  | Doctor         | Bengali  | Village     | L2       |\n",
    "| C2       | 60000  | Carpenter      | Hindi    | Suburban    | L2       |\n",
    "| C3       | 80000  | Doctor         | Bhojpuri | Metropolitan | L2       |\n",
    "| C5       | 80000  | Data Scientist | Hindi    | Small Town  | L1       |\n",
    "| C4       | 50000  | Data Scientist | Hindi    | Small Town  | ?        |\n",
    "\n",
    "## Solution\n",
    "\n",
    "- Use the formula to apply min-max normalization each income value:\n",
    "\n",
    "$ \\text{Normalized Income} = \\frac{\\text{Income} - \\text{Min}(Income)}{\\text{Max}(Income) - \\text{Min}(Income)} $\n",
    "\n",
    "\n",
    "Locality as ordinal variable with ranking order of [Village = 1, Small Town = 2, Suburban = 3, Metropolitan = 4].\n",
    "   - Use the formula to normalize each locality value:\n",
    "$ \\text{Normalized Ranked Locality} = \\frac{\\text{Ranked Locality} - \\text{Min}(Ranked Locality)}{\\text{Max}(Ranked Locality) - \\text{Min}(Ranked Locality)} $\n",
    "\n",
    "\n",
    "| Customer | Income | Normalized Income | Profession     | Region   | Locality    | Ranked Locality | Normalized Scaled Ranked Locality | Category |\n",
    "|----------|--------|-------------------|----------------|----------|-------------|-----------------|------------------------|----------|\n",
    "| C0       | 60000  | 0.33              | Doctor         | Hindi    | Village     | 1               | 0                      | L1       |\n",
    "| C1       | 70000  | 0.67              | Doctor         | Bengali  | Village     | 1               | 0                      | L2       |\n",
    "| C2       | 60000  | 0.33              | Carpenter      | Hindi    | Suburban    | 3               | 0.67                   | L2       |\n",
    "| C3       | 80000  | 1.00              | Doctor         | Bhojpuri | Metropolitan | 4               | 1.00                   | L2       |\n",
    "| C5       | 80000  | 1.00              | Data Scientist | Hindi    | Small Town  | 2               | 0.33                   | L1       |\n",
    "| C4       | 50000  | 0.00              | Data Scientist | Hindi    | Small Town  | 2               | 0.33                   | ?        |\n",
    "\n",
    "\n",
    "To calculate distance between (C4, C2)\n",
    "= distance w.r.t to numerical attributes + distance w.r.t Nominal attributes\n",
    "= Euclidean distance on (Income, Scaled Locality ) + Categorical distance on attributes (Profession, Region)\n",
    "\n",
    "Euclidean distance on (Income, Scaled Locality )\n",
    "= $\\sqrt{(Incomec4 − Incomec2)^2+(Localityc4 − Locailityc2)^2}$\n",
    "\n",
    "= $\\sqrt{(0 − 0.33)^2+(0.33 − 0.67)^2}$ = 0.47\n",
    "\n",
    "The attributes for C4 and C2:\n",
    "\n",
    "- C4: Data Scientist, Hindi\n",
    "- C2: Carpenter, Hindi\n",
    "\n",
    "Here, we have two categorical features: Profession and Region. \n",
    "\n",
    "For Profession:\n",
    "- C4 and C2 have different values (Data Scientist and Carpenter). Therefore, this feature does not match.\n",
    "\n",
    "For Region:\n",
    "- Both C4 and C2 have the same value (Hindi). Therefore, this feature matches.\n",
    "\n",
    "Categorical distance on attributes (Profession, Region)\n",
    " = $ \\frac{\\text{No. of categorical features} - \\text{No. of matches between C4 and C2}}{\\text{No. of categorical features}} $\n",
    " \n",
    " = $ \\frac{\\text{2} - \\text{1}}{\\text{2}} $ = 0.5 \n",
    "\n",
    " = 0.47 + 0.5 = 0.97\n",
    "\n",
    "Calculate weight assigned to each attribute.\n",
    "- Kernel function: $K(d(x_q, x_i)) = \\frac{1}{d(x_q, x_i)^2}$.\n",
    "\n",
    "= $\\frac{1}{0.97^2}$ = 1.06\n",
    "\n",
    "| Customer | Distance | Weight | Category |  Rank  |\n",
    "|----------|----------|--------|----------|--------|\n",
    "| C0       | 0.97     | 1.06   | L1       |  4\n",
    "| C1       | 1.75     | 0.33   | L2       |  2\n",
    "| C2       | 0.97     | 1.06   | L2       |  4\n",
    "| C3       | 2.20     | 0.21   | L2       |  1\n",
    "| C5       | 1.00     | 1.00   | L1       |  3\n",
    "\n",
    "Total Weight of L1 = 1.06 + 1.00 = 2.06\n",
    "Total Weight of L2 = 0.33 + 1.06 + 0.21 = 1.60\n",
    "\n",
    "**Inference** : Without weighted KNN, L2 is the majority voted class.\n",
    "But with weighted distances, L1 class instances are more similar to C4 (L1 > L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Locally Weighted Linear Regression**\n",
    "\n",
    "- Locally Weighted Linear Regression (LWLR) is a non-parametric regression technique used for making predictions based on locally weighted least squares.\n",
    "- LWLR is an extension of traditional linear regression that adapts to the local structure of the data.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - LWLR uses a kernel function to assign weights to each data point in the dataset.\n",
    "   - The kernel function determines the weight assigned to each data point based on its distance from the query point.\n",
    "   - Common kernel functions include Gaussian (or Radial Basis Function), triangular, and Epanechnikov kernels.\n",
    "\n",
    "2. **Weighted Least Squares:**\n",
    "   - Once the weights are assigned using the kernel function, LWLR performs a weighted least squares regression.\n",
    "   - Weighted least squares regression fits a linear model by giving higher importance to data points with higher weights.\n",
    "   - Unlike ordinary least squares regression, where all data points have equal importance, weighted least squares accounts for the proximity of data points to the query point.\n",
    "\n",
    "3. **Local Prediction:**\n",
    "   - LWLR predicts the output for a new query point by fitting a linear model to the subset of data points that are closest to the query point.\n",
    "   - The weights assigned by the kernel function ensure that data points closer to the query point have a greater influence on the regression model, while points farther away have less influence.\n",
    "   - This local prediction approach allows LWLR to capture non-linear relationships and adapt to the local structure of the data.\n",
    "\n",
    "4. **Adaptive Nature:**\n",
    "   - One of the main advantages of LWLR is its adaptive nature.\n",
    "   - LWLR adapts to the local structure of the data by assigning higher weights to nearby data points and lower weights to distant ones.\n",
    "   - This adaptability makes LWLR suitable for datasets with complex, non-linear relationships or varying patterns across different regions of the feature space.\n",
    "\n",
    "5. **Hyperparameter Tuning:**\n",
    "   - LWLR involves tuning hyperparameters such as the bandwidth parameter of the kernel function.\n",
    "   - The bandwidth parameter determines the width of the neighborhood around the query point that is considered for local regression.\n",
    "   - Choosing an appropriate bandwidth is crucial as it affects the trade-off between bias and variance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "| Product | Size | Rating (1-10) | Product Price (scaled down) |\n",
    "|---------|------|---------------|-----------------------------|\n",
    "| A       | 10   | 5             | 5                           |\n",
    "| B       | 5    | 2             | 7                           |\n",
    "| C       | 2    | 5             | 2                           |\n",
    "| D       | 7    | 2             | 3                           |\n",
    "| E       | 6    | 3             | 5                           |\n",
    "| F       | 15   | 8             | 4                           |\n",
    "| G       | 5    | 30            | 9                           |\n",
    "| H       | 20   | 8             | 8                           |\n",
    "| I       | 6    | 10            | 9                           |\n",
    "| J       | 2    | 2             | 3                           |\n",
    "| K       | 10   | 4             | 9                           |\n",
    "| L       | 13   | 6             | 8                           |\n",
    "\n",
    "## 1. Consider the following training set in 2-dimensional Euclidean space. What is the prize of the product with size 12 if 7-NN is considered? Use the kernel of the form K(d(xq,xi)) = 1/ d(xq,xi)^2. Use the linear regression of the form y = 2 + 1.5 x and the learning rate of 0.6 to apply the gradient descent and update the weights at the end of first iteration. Use Manhattan Distance for the distance calculation.\n",
    "   \n",
    "### Solution\n",
    "Sure, let's incorporate the calculations directly into the table:\n",
    "\n",
    "| Product | Size(X) | Manhattan Distance to Size 12 (12 - x) | Price Prediction (y = 2 + 1.5x) |\n",
    "|---------|------|--------------------------------|---------------------------------|\n",
    "| A       | 10   | 2                              | 17  = 2 + 1.5.10                            |\n",
    "| B       | 5    | 7                              | 9.5                             |\n",
    "| C       | 2    | 10                             | 5                               |\n",
    "| D       | 7    | 5                              | 12.5                            |\n",
    "| E       | 6    | 6                              | 11                              |\n",
    "| F       | 15   | 3                              | 24.5                            |\n",
    "| G       | 5    | 7                              | 9.5                             |\n",
    "| H       | 20   | 8                              | 29                              |\n",
    "| I       | 6    | 6                              | 11                              |\n",
    "| J       | 2    | 10                             | 5                               |\n",
    "| K       | 10   | 2                              | 17                              |\n",
    "| L       | 13   | 1                              | 21.5                            |\n",
    "\n",
    "To rank the products using the 7-nearest neighbors (KNN7) approach, we'll first identify the 7 products with the smallest Manhattan distance to the size 12. Then, we'll rank these products based on their predicted prices using linear regression.\n",
    "\n",
    "1. Select the 7 nearest neighbors based on the smallest Manhattan distances: K, A, F, D, E, L, G.\n",
    "\n",
    "2. Use linear regression to predict the price for each of these 7 products:\n",
    "\n",
    "| Product | Size | Manhattan Distance to Size 12 | Price Prediction (y = 2 + 1.5x) |\n",
    "|---------|------|--------------------------------|---------------------------------|\n",
    "| K       | 10   | 2                              | 17                              |\n",
    "| A       | 10   | 2                              | 17                              |\n",
    "| F       | 15   | 3                              | 24.5                            |\n",
    "| D       | 7    | 5                              | 12.5                            |\n",
    "| E       | 6    | 6                              | 11                              |\n",
    "| L       | 13   | 1                              | 21.5                            |\n",
    "| G       | 5    | 7                              | 9.5                             |\n",
    "\n",
    "Average Price:\n",
    "$\n",
    "\\text{Average Price} = \\frac{21.5 + 17 + 24.5 + 12.5 + 11 + 17 + 9.5}{7} = \\frac{113}{7} \\approx 16.14\n",
    "$\n",
    "\n",
    "Therefore, the predicted price of the product with size 12 using 7-NN and linear regression is approximately \\$16.14.\n",
    "\n",
    "Regarding the gradient descent part of the problem, the update of weights at the end of the first iteration depends on the initial weights, the learning rate, and the error in the prediction. Since the problem didn't provide these details, we'll focus on the k-NN prediction as described above.\n",
    "\n",
    "## 2. Try to scale all the features with min-max of [0-5] and redo the modelling under previous question. Observe to Interpret the influence of the scaling\n",
    "\n",
    "### Solution\n",
    "\n",
    "1. **Min-Max Scaling Formula**:\n",
    "   - Given a feature $ x $ with minimum value $ x_{\\text{min}} $ and maximum value $ x_{\\text{max}} $:\n",
    "     $ \\text{Scaled} \\, x = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}} \\times (\\text{new\\_max} - \\text{new\\_min}) + \\text{new\\_min} $\n",
    "\n",
    "   - For scaling to the range [0-5]:\n",
    "     $ \\text{Scaled} \\, x = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}} \\times 5 $\n",
    "\n",
    "\n",
    "Sure, let's apply min-max scaling to the given table:\n",
    "\n",
    "Given:\n",
    "- Size: Min = 2, Max = 20\n",
    "- Rating: Min = 2, Max = 10\n",
    "- Product Price (scaled down): Min = 2, Max = 9\n",
    "\n",
    "Using the min-max scaling formula:\n",
    "\n",
    "1. For Size:\n",
    "   $ \\text{Scaled Size} = \\frac{\\text{Size} - 2}{20 - 2} \\times 5 $\n",
    "\n",
    "2. For Rating:\n",
    "   $ \\text{Scaled Rating} = \\frac{\\text{Rating} - 2}{10 - 2} \\times 5 $\n",
    "\n",
    "3. For Product Price (scaled down):\n",
    "   $ \\text{Scaled Price} = \\frac{\\text{Price} - 2}{9 - 2} \\times 5 $\n",
    "\n",
    "Let's calculate and add the scaled features to the original table:\n",
    "\n",
    "| Product | Size | Scaled Size | Rating | Scaled Rating | Price (scaled down) | Scaled Price |\n",
    "|---------|------|-------------|--------|---------------|---------------------|--------------|\n",
    "| A       | 10   | $ \\frac{10 - 2}{20 - 2} \\times 5 = 2.5 $ | 5      | $ \\frac{5 - 2}{10 - 2} \\times 5 = 1.875 $ | 5     | $ \\frac{5 - 2}{9 - 2} \\times 5 = 1.875 $ |\n",
    "| B       | 5    | $ \\frac{5 - 2}{20 - 2} \\times 5 = 1.25 $ | 2      | $ \\frac{2 - 2}{10 - 2} \\times 5 = 0 $ | 7     | $ \\frac{7 - 2}{9 - 2} \\times 5 = 5 $ |\n",
    "| C       | 2    | $ \\frac{2 - 2}{20 - 2} \\times 5 = 0 $ | 5      | $ \\frac{5 - 2}{10 - 2} \\times 5 = 1.875 $ | 2     | $ \\frac{2 - 2}{9 - 2} \\times 5 = 0 $ |\n",
    "| D       | 7    | $ \\frac{7 - 2}{20 - 2} \\times 5 = 1.875 $ | 2      | $ \\frac{2 - 2}{10 - 2} \\times 5 = 0 $ | 3     | $ \\frac{3 - 2}{9 - 2} \\times 5 = 0.714 $ |\n",
    "| E       | 6    | $ \\frac{6 - 2}{20 - 2} \\times 5 = 1.5 $ | 3      | $ \\frac{3 - 2}{10 - 2} \\times 5 = 0.625 $ | 5     | $ \\frac{5 - 2}{9 - 2} \\times 5 = 1.875 $ |\n",
    "| F       | 15   | $ \\frac{15 - 2}{20 - 2} \\times 5 = 4.375 $ | 8      | $ \\frac{8 - 2}{10 - 2} \\times 5 = 3.75 $ | 4     | $ \\frac{4 - 2}{9 - 2} \\times 5 = 0.714 $ |\n",
    "| G       | 5    | $ \\frac{5 - 2}{20 - 2} \\times 5 = 1.25 $ | 30     | $ \\frac{30 - 2}{10 - 2} \\times 5 = 17.5 $ | 9     | $ \\frac{9 - 2}{9 - 2} \\times 5 = 5 $ |\n",
    "| H       | 20   | $ \\frac{20 - 2}{20 - 2} \\times 5 = 5 $ | 8      | $ \\frac{8 - 2}{10 - 2} \\times 5 = 3.75 $ | 8     | $ \\frac{8 - 2}{9 - 2} \\times 5 = 1.875 $ |\n",
    "| I       | 6    | $ \\frac{6 - 2}{20 - 2} \\times 5 = 1.5 $ | 10     | $ \\frac{10 - 2}{10 - 2} \\times 5 = 5 $ | 9     | $ \\frac{9 - 2}{9 - 2} \\times 5 = 5 $ |\n",
    "| J       | 2    | $ \\frac{2 - 2}{20 - 2} \\times 5 = 0 $ | 2      | $ \\frac{2 - 2}{10 - 2} \\times 5 = 0 $ | 3     | $ \\frac{3 - 2}{9 - 2} \\times 5 = 0.714 $ |\n",
    "\n",
    "\n",
    "Now, let's perform the KNN modeling and linear regression as described above. After that, we'll rank the products based on their predicted Prices.\n",
    "\n",
    "## 3. Find the MSE of the K-NNs from the results of both above parts separately & state your observations\n",
    "### Solution\n",
    "1. Calculate the Mean Squared Error (MSE) using the formula:\n",
    "\n",
    "$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "Where:\n",
    "- $ n $ is the number of products\n",
    "- $ y_i $ is the actual \"Product Price\" for product $ i $\n",
    "- $ \\hat{y}_i $ is the predicted \"Product Price\" for product $ i $\n",
    "\n",
    "Let's calculate the MSE:\n",
    "\n",
    "$ \\text{MSE} = \\frac{1}{12} \\left[ (5 - 16.14)^2 + (5 - 16.14)^2 + (5 - 16.14)^2 + ... \\right] $\n",
    "\n",
    "##### Observation: The MSE value indicates the overall accuracy of the predictive model. A lower MSE suggests that the model's predictions are closer to the actual prices, while a higher MSE indicates larger prediction errors.\n",
    "\n",
    "## 4. If the Product with features (13,6) and (30,4) are outliers in the table below then interpret the effect of the same in the K- NN modelling, before and after removing them with numerical justication.\n",
    "### Solution\n",
    "\n",
    "1. Step 1 - Calculate MSE of the original data\n",
    "   1. Effect: With outliers present, the K-NN algorithm may produce less accurate predictions as the outliers may dominate the nearest neighbor selection process, leading to biased results.\n",
    "2. Step 1 - Calculate MSE after removing the outliers form the original data\n",
    "\n",
    "Observation: \n",
    "   - Before removing outliers, these data points may significantly affect the determination of nearest neighbors, leading to potentially biased predictions.\n",
    "   - After removing outliers, the K-NN algorithm can better capture the underlying patterns in the data without the influence of extreme values, resulting in more reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_samples=9 should be >= n_clusters=10.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_values:\n\u001b[1;32m     13\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     inertia_values\u001b[38;5;241m.\u001b[39mappend(kmeans\u001b[38;5;241m.\u001b[39minertia_)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Plot the elbow curve\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1146\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1138\u001b[0m     X,\n\u001b[1;32m   1139\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1143\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1144\u001b[0m )\n\u001b[0;32m-> 1146\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[1;32m   1148\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:947\u001b[0m, in \u001b[0;36mKMeans._check_params\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# n_clusters\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters:\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be >= n_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# tol\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol \u001b[38;5;241m=\u001b[39m _tolerance(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol)\n",
      "\u001b[0;31mValueError\u001b[0m: n_samples=9 should be >= n_clusters=10."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features (height and weight) and labels (class)\n",
    "X = [(61, 190), (62, 182), (57, 185), (51, 167), (69, 176), (56, 174), (60, 173), (55, 172), (65, 172)]\n",
    "y = ['Under-weight', 'Normal', 'Under-weight', 'Under-weight', 'Normal', 'Under-weight', 'Normal', 'Normal', 'Normal']\n",
    "\n",
    "# Calculate sum of squared distances for different values of k\n",
    "k_values = range(1, 11)\n",
    "inertia_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(data)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()\n",
    "\n",
    "# Create the KNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Predict the class for the incoming patient with weight = 58 Kg and Height = 180 cm\n",
    "new_patient = [(58, 180)]\n",
    "predicted_class = knn.predict(new_patient)\n",
    "\n",
    "print(\"Predicted class for the incoming patient:\", predicted_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
