{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree:**\n",
    "\n",
    "- **Definition:** Decision Tree is a popular supervised machine learning algorithm used for classification and regression tasks. It organizes data into a tree-like structure where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label (in classification) or a numerical value (in regression).\n",
    "\n",
    "- **Example:**\n",
    "  - Suppose we have a dataset of weather conditions and corresponding decisions to play tennis:\n",
    "    - **Outlook**: Sunny, Overcast, Rainy\n",
    "    - **Temperature**: Hot, Mild, Cool\n",
    "    - **Humidity**: High, Normal\n",
    "    - **Wind**: Weak, Strong\n",
    "    - **Decision**: Play, Don't Play\n",
    "\n",
    "| Outlook | Temperature | Humidity | Wind | Decision |\n",
    "|---------|-------------|----------|------|----------|\n",
    "| Sunny   | Hot         | High     | Weak | Don't Play  |\n",
    "| Sunny   | Hot         | High     | Strong | Don't Play  |\n",
    "| Overcast| Hot         | High     | Weak | Play  |\n",
    "| Rainy   | Mild        | High     | Weak | Play  |\n",
    "| Rainy   | Cool        | Normal   | Weak | Play  |\n",
    "| Rainy   | Cool        | Normal   | Strong | Don't Play |\n",
    "| Overcast| Cool        | Normal   | Strong | Play  |\n",
    "| Sunny   | Mild        | High     | Weak | Don't Play |\n",
    "| Sunny   | Cool        | Normal   | Weak | Play  |\n",
    "| Rainy   | Mild        | Normal   | Weak | Play  |\n",
    "| Sunny   | Mild        | Normal   | Strong | Play  |\n",
    "| Overcast| Mild        | High     | Strong | Play  |\n",
    "| Overcast| Hot         | Normal   | Weak | Play  |\n",
    "| Rainy   | Mild        | High     | Strong | Don't Play |\n",
    "\n",
    "- **Mathematical Formulas:**\n",
    "\n",
    "  1. **Entropy (H(S))**:\n",
    "     - Entropy measures the impurity or randomness in a dataset.\n",
    "     - Formula: $ H(S) = -\\sum_{i=1}^{m} p_i \\log_2(p_i) $\n",
    "     - Example: Suppose there are two classes, Yes and No, and the dataset has 9 instances of Yes and 5 instances of No.\n",
    "     - Calculation: $ H(S) = -(\\frac{9}{14} \\log_2(\\frac{9}{14}) + \\frac{5}{14} \\log_2(\\frac{5}{14})) $\n",
    "\n",
    "  2. **Information Gain (IG(A))**:\n",
    "     - Information Gain measures the effectiveness of an attribute in classifying the data.\n",
    "     - Formula: $ IG(S, A) = H(S) - \\sum_{i=1}^{n} \\frac{|S_i|}{|S|} H(S_i) $\n",
    "     - Example: To decide the splitting attribute at the root node, calculate the information gain for each attribute.\n",
    "\n",
    "  3. **Decision Rule**:\n",
    "     - Once the tree is constructed, decision rules at each node determine the traversal path.\n",
    "     - Example: If Outlook is Sunny, go left; if Overcast, go middle; if Rainy, go right.\n",
    "\n",
    "  4. **Pruning (Optional)**:\n",
    "     - Pruning removes branches that do not provide significant improvements.\n",
    "     - Example: Remove nodes with minimal impact on overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "### Taken from https://vtupluse.com/\n",
    "\n",
    "<img src=\"images/ID3-ex1-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-3.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-5.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-6.jpg\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-7.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-8.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-9.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-10.jpg\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-11.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-12.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-13.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-14.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "### Taken from https://vtupluse.com/\n",
    "\n",
    "<img src=\"images/ID3-ex2-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-5.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-6.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-7.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-8.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-9.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-10.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-11.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-12.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-13.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-14.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-15.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-16.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-16.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-18.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-19.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Overfitting in Decision Trees**\n",
    "\n",
    "Imagine you're training a decision tree to classify apples and oranges based on their size and color. Overfitting happens when the tree gets too hung up on the specifics of the training data, memorizing every detail  including some random quirks, instead of learning the general patterns that differentiate apples and oranges.\n",
    "\n",
    "**Visualization: Underfitting vs Overfitting**\n",
    "\n",
    "Let's look at two scenarios:\n",
    "\n",
    "1. **Underfitting:** The tree is too simple, with just one or two splits. It might classify everything as green, failing to capture the key differences between the fruits. \n",
    "\n",
    "2. **Overfitting:** The tree keeps splitting on increasingly specific rules, like \"diameter greater than 2.5cm and slightly yellow-ish rind\" for apple. This might perfectly classify the training data, but it won't work well for unseen apples (maybe a slightly redder one) or large oranges.\n",
    "\n",
    "<img src=\"images/DT_1.webp\">\n",
    "\n",
    "**Why Overfitting is Bad**\n",
    "\n",
    "The overfitting tree becomes like someone who memorized every question on a practice test but can't answer new ones. It performs well on the training data but struggles with unseen data, which is the real test of a model's generalizability.\n",
    "\n",
    "**Combating Overfitting**\n",
    "\n",
    "Here are some ways to prevent decision trees from overfitting:\n",
    "\n",
    "* **Limit Tree Depth:**  Control the complexity by setting a maximum depth for the tree, stopping it from splitting endlessly.\n",
    "* **Pruning:** Strategically remove unnecessary branches that capture noise in the data.\n",
    "* **Regularization:** Introduce penalties for overly complex trees, encouraging simpler models.\n",
    "<img src=\"images/DT_2.png\">\n",
    "\n",
    "By applying these techniques, you can build decision trees that are more accurate on unseen data and perform better in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Alternative to Entropy: Gini Index**\n",
    "\n",
    "- **Definition**: Gini Index is another impurity measure used in decision tree algorithms, particularly for binary splits. It measures the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node.\n",
    "\n",
    "- **Mathematical Formulas**:\n",
    "\n",
    "  1. **Gini Index (Gini(S))**:\n",
    "     - Gini index for a dataset S with m classes:\n",
    "     $ Gini(S) = 1 - \\sum_{i=1}^{m} p_i^2 $\n",
    "     where $ p_i $ is the proportion of instances belonging to class $ i $ in S.\n",
    "     \n",
    "  2. **Gini Index for Splitting (Gini\\_split(A))**:\n",
    "     - Gini index for splitting a dataset S based on attribute A:\n",
    "     $ Gini\\_split(A) = \\sum_{j} \\frac{|S_j|}{|S|} Gini(S_j) $\n",
    "     where $ S_j $ is the subset of S after splitting on attribute A.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "  - Suppose we have a binary classification problem with two classes, Yes and No.\n",
    "  - Consider a node with 10 instances, where 7 instances belong to class Yes and 3 instances belong to class No.\n",
    "  - Calculation of Gini Index:\n",
    "    $ Gini(S) = 1 - \\left(\\left(\\frac{7}{10}\\right)^2 + \\left(\\frac{3}{10}\\right)^2\\right) = 1 - \\left(\\frac{49}{100} + \\frac{9}{100}\\right) = 1 - \\frac{58}{100} = \\frac{42}{100} = 0.42 $\n",
    "\n",
    "- **Binary Splits**:\n",
    "  - Gini Index is particularly useful for binary splits, where an attribute divides the dataset into two subsets.\n",
    "  - At each node, the algorithm calculates the Gini Index for each possible split and chooses the one with the lowest index.\n",
    "\n",
    "- **Comparison with Entropy**:\n",
    "  - Both Gini Index and Entropy are impurity measures used in decision trees.\n",
    "  - Gini Index tends to be faster to compute than entropy because it doesn't involve logarithmic calculations.\n",
    "  - In practice, both measures often lead to similar results, but there may be slight differences in performance depending on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "### Taken from https://vtupluse.com/\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-3.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-5.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-6.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-7.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-8.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-9.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-10.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-11.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-12.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-13.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-14.png\" width=\"100%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
