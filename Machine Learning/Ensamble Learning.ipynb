{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "In ensemble learning, bagging (Bootstrap Aggregating) is a technique used to improve the stability and accuracy of machine learning algorithms, especially for models that are sensitive to the specific training data. The basic idea behind bagging is to train multiple instances of the same learning algorithm on different subsets of the training data and then combine their predictions.\n",
    "\n",
    "Here's how bagging works:\n",
    "\n",
    "1. **Bootstrap Sampling**: From the original training dataset, multiple random samples (called bootstrap samples) are drawn with replacement. Each bootstrap sample is typically of the same size as the original dataset, but some instances may be repeated while others may be left out.\n",
    "\n",
    "2. **Model Training**: A base learning algorithm (often referred to as a \"weak learner\") is trained on each bootstrap sample. This could be any machine learning algorithm, such as decision trees, neural networks, or support vector machines.\n",
    "\n",
    "3. **Prediction Aggregation**: Once all base models are trained, predictions are made for each data point in the test set by each individual model. For regression problems, the final prediction may be the average of predictions from all models, while for classification problems, it could be the majority vote or mode of the predictions.\n",
    "\n",
    "The key advantages of bagging include:\n",
    "- Reducing variance: By training multiple models on different subsets of the data, bagging helps to reduce the variance of the final model, which can be especially useful for unstable models prone to overfitting.\n",
    "- Handling outliers and noise: Bagging can improve model robustness by reducing the impact of outliers and noise in the training data.\n",
    "- Increasing accuracy: By combining predictions from multiple models, bagging often leads to more accurate predictions compared to using a single model.\n",
    "\n",
    "Popular algorithms based on bagging include Random Forest for decision trees and Bagged Aggregation for various base learners.\n",
    "\n",
    "Overall, bagging is a powerful technique in ensemble learning that helps to create more robust and accurate models by leveraging the diversity of multiple base models trained on different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "<img src=\"images/bagging-ex1-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-3.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-5.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-6.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-7.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-8.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-9.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-10.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-11.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-12.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/bagging-ex1-13.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "**Step 1: Starting with a Simple Model:**  \n",
    "- Imagine you have a bunch of students, each trying to solve a problem.\n",
    "- Initially, you give each student an equal opportunity to solve the problem, meaning they all have the same weight or importance.\n",
    "- Each student (or model) comes up with a simple way to solve the problem.\n",
    "\n",
    "**Step 2: Evaluating Performance:**  \n",
    "- After each student solves the problem, you check how well they did.\n",
    "- Some students might solve the problem better than others.\n",
    "- You pay more attention to the students who did well.\n",
    "\n",
    "**Step 3: Adjusting for Mistakes:**  \n",
    "- Now, you give more attention to the students who didn’t do well. \n",
    "- You ask them to focus more on the parts of the problem they struggled with.\n",
    "- You also adjust the weight of the problem itself to focus on the parts that are harder.\n",
    "\n",
    "**Step 4: Combining Results:**  \n",
    "- Finally, you combine the solutions from all students.\n",
    "- But you give more weight to the solutions of the students who did better.\n",
    "- This way, you get a combined solution that is better than any individual student's solution.\n",
    "\n",
    "### Mathematical Intuition:\n",
    "\n",
    "**Step 1: Starting with a Simple Model:**  \n",
    "- Each student represents a simple model (e.g., a decision tree) that tries to solve the problem.\n",
    "- Initially, each model has equal importance or weight.\n",
    "\n",
    "**Step 2: Evaluating Performance:**  \n",
    "- After each model tries to solve the problem, we calculate how well it performed using a metric like accuracy.\n",
    "- We identify the errors made by each model.\n",
    "\n",
    "**Step 3: Adjusting for Mistakes:**  \n",
    "- We give more weight to the errors made by the models.\n",
    "- This means the next model will focus more on the examples that previous models got wrong.\n",
    "- Additionally, we adjust the weight of each example, so the next model focuses more on the harder examples.\n",
    "\n",
    "**Step 4: Combining Results:**  \n",
    "- Finally, we combine the solutions from all models, giving more weight to the solutions of the models that performed better.\n",
    "- Mathematically, the combined solution is a weighted sum of individual model predictions, where weights are based on model performance.\n",
    "\n",
    "### Formulas:\n",
    "\n",
    "Let's denote:\n",
    "- $ D_t(i) $ as the weight of the $ i $-th training example at iteration $ t $.\n",
    "- $ h_t(x_i) $ as the prediction of the $ t $-th weak learner (model) for the $ i $-th training example $ x_i $.\n",
    "- $ \\epsilon_t $ as the weighted error of the $ t $-th weak learner.\n",
    "- $ \\alpha_t $ as the weight given to the $ t $-th weak learner's prediction in the final ensemble.\n",
    "\n",
    "Then, AdaBoost works as follows:\n",
    "\n",
    "1. **Weighted Error of Weak Learner:**\n",
    "   $ \\epsilon_t = \\frac{\\sum_{i=1}^{N} D_t(i) \\cdot \\text{error}(h_t(x_i))}{\\sum_{i=1}^{N} D_t(i)} $\n",
    "   \n",
    "2. **Weight of Weak Learner:**\n",
    "   $ \\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - \\epsilon_t}{\\epsilon_t} \\right) $\n",
    "   \n",
    "3. **Update Weights of Examples:**\n",
    "   $ D_{t+1}(i) = \\frac{D_t(i) \\cdot \\exp(-\\alpha_t \\cdot y_i \\cdot h_t(x_i))}{\\sum_{j=1}^{N} D_t(j)} $\n",
    "   \n",
    "   Where $ y_i $ is the true label of example $ x_i $, and $ h_t(x_i) $ is the prediction of the $ t $-th weak learner for example $ x_i $.\n",
    "\n",
    "4. **Final Prediction:**\n",
    "   $ H(x) = \\text{sign} \\left( \\sum_{t=1}^{T} \\alpha_t \\cdot h_t(x) \\right) $\n",
    "\n",
    "In simple terms, these formulas capture the iterative process of AdaBoost: evaluating model performance, assigning weights to models based on their performance, updating example weights to focus on harder examples, and combining model predictions into a final ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "Given data\n",
    "\n",
    "|x     | y   | Weight|\n",
    "------|------|----------\n",
    "0.1   | 1   | 0.1\n",
    "0.2   | 1   | 0.1\n",
    "0.3   | 1   | 0.1\n",
    "0.4   | -1  | 0.1\n",
    "0.5   | -1  | 0.1\n",
    "0.6   | -1  | 0.1\n",
    "0.7   | -1  | 0.1\n",
    "0.8   | 1   | 0.1\n",
    "0.9   | 1   | 0.1\n",
    "1     | 1   | 0.1\n",
    "\n",
    "## **Iteration 1**\n",
    "Step 1: Initialize weights\n",
    "Initially, each sample has equal weight, which is 1/10.\n",
    "\n",
    "Step 2: For each iteration:\n",
    "\n",
    "### a. Train a weak learner: We'll use a decision stump as our weak learner, which is essentially a decision tree with a single split.\n",
    "\n",
    "Let's say for the first iteration, the decision stump splits the data into two parts based on a threshold:\n",
    "\n",
    "```\n",
    "Threshold = 0.45\n",
    "Predictions:\n",
    "For x < 0.45: Predict 1\n",
    "For x >= 0.45: Predict -1\n",
    "```\n",
    "**Before Prediction:**\n",
    "\n",
    "| x    | y    | Weight |\n",
    "|------|------|--------|\n",
    "| 0.1  | 1    | 0.1    |\n",
    "| 0.2  | 1    | 0.1    |\n",
    "| 0.3  | 1    | 0.1    |\n",
    "| 0.4  | -1   | 0.1    |\n",
    "| 0.5  | -1   | 0.1    |\n",
    "| 0.6  | -1   | 0.1    |\n",
    "| 0.7  | -1   | 0.1    |\n",
    "| 0.8  | 1    | 0.1    |\n",
    "| 0.9  | 1    | 0.1    |\n",
    "| 1    | 1    | 0.1    |\n",
    "\n",
    "**After Prediction:**\n",
    "\n",
    "| x    | y    | Weight | Prediction |\n",
    "|------|------|--------|------------|\n",
    "| 0.1  | 1    | 0.1    | 1          |\n",
    "| 0.2  | 1    | 0.1    | 1          |\n",
    "| 0.3  | 1    | 0.1    | 1          |\n",
    "| 0.4  | -1   | 0.1    | 1          |\n",
    "| 0.5  | -1   | 0.1    | -1         |\n",
    "| 0.6  | -1   | 0.1    | -1         |\n",
    "| 0.7  | -1   | 0.1    | -1         |\n",
    "| 0.8  | 1    | 0.1    | -1         |\n",
    "| 0.9  | 1    | 0.1    | -1         |\n",
    "| 1    | 1    | 0.1    | -1         |\n",
    "\n",
    "In the \"After Prediction\" table, there are four misclassified samples:\n",
    "\n",
    "1. For x = 0.4, the true label is -1, but the prediction is 1.\n",
    "2. For x = 0.8, the true label is 1, but the prediction is -1.\n",
    "3. For x = 0.9, the true label is 1, but the prediction is -1.\n",
    "4. For x = 1, the true label is 1, but the prediction is -1.\n",
    "\n",
    "These samples are misclassified because they fall on the side of the threshold opposite to their true label. Let me know if you have any further questions!\n",
    "\n",
    "### b. Compute error: Error is computed as the weighted sum of misclassified samples.\n",
    "\n",
    "To calculate the error, we sum the weights of the misclassified samples and then divide by the sum of all weights. \n",
    "\n",
    "```\n",
    "Error = (Sum of weights of misclassified samples) / (Sum of all weights)\n",
    "```\n",
    "\n",
    "From the \"Before Prediction\" table:\n",
    "- Weights of misclassified samples: 0.1 (for x = 0.4), 0.1 (for x = 0.8), 0.1 (for x = 0.9), 0.1 (for x = 1)\n",
    "- Sum of weights of misclassified samples: 0.1 + 0.1 + 0.1 + 0.1 = 0.4\n",
    "\n",
    "Sum of all weights: 1.0\n",
    "\n",
    "```\n",
    "Error = 0.4 / 1.0 = 0.4\n",
    "```\n",
    "\n",
    "So, the error is 0.4 or 40%. This means 40% of the samples are misclassified based on the current weak learner's threshold of 0.45. Let me know if you need further assistance!\n",
    "\n",
    "### c. Compute weight of the weak learner: We use the error to compute the weight of the weak learner in the final model.\n",
    "\n",
    "To compute the weight of the weak learner in AdaBoost, we use the error of the weak learner. The formula to compute the weight of the weak learner is:\n",
    "\n",
    "$ \\text{Weight of weak learner} = \\frac{1}{2} \\ln\\left(\\frac{1 - \\text{error}}{\\text{error}}\\right) $\n",
    "\n",
    "Given that the error is 0.4 (as calculated previously):\n",
    "\n",
    "$ \\text{Weight of weak learner} = \\frac{1}{2} \\ln\\left(\\frac{1 - 0.4}{0.4}\\right) $\n",
    "\n",
    "$ \\text{Weight of weak learner} = \\frac{1}{2} \\ln\\left(\\frac{0.6}{0.4}\\right) $\n",
    "\n",
    "$ \\text{Weight of weak learner} = \\frac{1}{2} \\ln(1.5) $\n",
    "\n",
    "$ \\text{Weight of weak learner} = \\frac{1}{2} \\times 0.4055 $\n",
    "\n",
    "$ \\text{Weight of weak learner} = 0.2028 $\n",
    "\n",
    "So, the weight of the weak learner is approximately 0.2028. Let me know if you have any further questions!\n",
    "\n",
    "### d. Update weights: We update the weights of the samples, giving higher weight to misclassified samples.\n",
    "\n",
    "To update the weights of the samples, we use the following formulas:\n",
    "\n",
    "For correctly classified samples:\n",
    "$ \\text{New weight}_i = \\text{Old weight}_i \\times e^{-\\text{weight of weak learner}} $\n",
    "\n",
    "For misclassified samples:\n",
    "$ \\text{New weight}_i = \\text{Old weight}_i \\times e^{\\text{weight of weak learner}} $\n",
    "\n",
    "Let's calculate the updated weights and present them in the form of a table:\n",
    "\n",
    "| x    | y    | Weight |   Weight (Updated) |\n",
    "|------|------|--------|-------------------|\n",
    "| 0.1  | 1    | 0.1    |0.1 * exp(-0.2028) ≈ 0.0899 |\n",
    "| 0.2  | 1    | 0.1    |0.1 * exp(-0.2028) ≈ 0.0899 |\n",
    "| 0.3  | 1    | 0.1    |0.1 * exp(-0.2028) ≈ 0.0899 |\n",
    "| 0.4  | -1   | 0.1    |0.1 * exp(0.2028) ≈ 0.1111 |\n",
    "| 0.5  | -1   | 0.1    |0.1 * exp(0.2028) ≈ 0.1111 |\n",
    "| 0.6  | -1   | 0.1    |0.1 * exp(0.2028) ≈ 0.1111 |\n",
    "| 0.7  | -1   | 0.1    |0.1 * exp(0.2028) ≈ 0.1111 |\n",
    "| 0.8  | 1    | 0.1    |0.1 * exp(-0.2028) ≈ 0.0899 |\n",
    "| 0.9  | 1    | 0.1    |0.1 * exp(-0.2028) ≈ 0.0899 |\n",
    "| 1    | 1    | 0.1    |0.1 * exp(-0.2028) ≈ 0.0899 |\n",
    "\n",
    "## **Iteration 2**\n",
    "For the second iteration of AdaBoost, we follow the same steps as before:\n",
    "\n",
    "1. Train a weak learner.\n",
    "2. Compute the error.\n",
    "3. Compute the weight of the weak learner.\n",
    "4. Update the weights of the samples.\n",
    "\n",
    "Let's continue with the updated weights from the first iteration:\n",
    "\n",
    "**Updated Weights from First Iteration:**\n",
    "\n",
    "| x    | y    | Weight (Updated) |\n",
    "|------|------|-------------------|\n",
    "| 0.1  | 1    | 0.0899 |\n",
    "| 0.2  | 1    | 0.0899 |\n",
    "| 0.3  | 1    | 0.0899 |\n",
    "| 0.4  | -1   | 0.1111 |\n",
    "| 0.5  | -1   | 0.1111 |\n",
    "| 0.6  | -1   | 0.1111 |\n",
    "| 0.7  | -1   | 0.1111 |\n",
    "| 0.8  | 1    | 0.0899 |\n",
    "| 0.9  | 1    | 0.0899 |\n",
    "| 1    | 1    | 0.0899 |\n",
    "\n",
    "Now, let's proceed with the second iteration:\n",
    "\n",
    "**Weak Learner for Second Iteration:**\n",
    "Let's say we choose the split at x = 0.25.\n",
    "\n",
    "**Error Calculation:**\n",
    "```\n",
    "Error = (Sum of weights of misclassified samples) / (Sum of all weights)\n",
    "```\n",
    "From the updated weights:\n",
    "- Weights of misclassified samples: 0.0899 (for x = 0.1), 0.0899 (for x = 0.2)\n",
    "- Sum of weights of misclassified samples: 0.0899 + 0.0899 = 0.1798\n",
    "\n",
    "Sum of all weights: 1.0\n",
    "\n",
    "```\n",
    "Error = 0.1798 / 1.0 = 0.1798\n",
    "```\n",
    "\n",
    "**Weight of the Weak Learner:**\n",
    "```\n",
    "Weight of weak learner = 0.5 * ln((1 - error) / error)\n",
    "Weight of weak learner = 0.5 * ln((1 - 0.1798) / 0.1798)\n",
    "Weight of weak learner ≈ 0.881\n",
    "```\n",
    "\n",
    "**Updating Weights:**\n",
    "\n",
    "For correctly classified samples:\n",
    "$ \\text{New weight}_i = \\text{Old weight}_i \\times e^{-\\text{weight of weak learner}} $\n",
    "\n",
    "For misclassified samples:\n",
    "$ \\text{New weight}_i = \\text{Old weight}_i \\times e^{\\text{weight of weak learner}} $\n",
    "\n",
    "We'll update the weights accordingly and present them in a table.\n",
    "Sure, let's update the weights using the formulas mentioned earlier and present them in a table:\n",
    "\n",
    "| x    | y    | Weight (First Iteration) | Weight (Updated) |\n",
    "|------|------|---------------------------|-------------------|\n",
    "| 0.1  | 1    | 0.0899                    |0.0899 * exp(-0.881) ≈ 0.0451 |\n",
    "| 0.2  | 1    | 0.0899                    |0.0899 * exp(-0.881) ≈ 0.0451 |\n",
    "| 0.3  | 1    | 0.0899                    |0.0899 * exp(-0.881) ≈ 0.0451 |\n",
    "| 0.4  | -1   | 0.1111                    |0.1111 * exp(0.881) ≈ 0.2186 |\n",
    "| 0.5  | -1   | 0.1111                    |0.1111 * exp(0.881) ≈ 0.2186 |\n",
    "| 0.6  | -1   | 0.1111                    |0.1111 * exp(0.881) ≈ 0.2186 |\n",
    "| 0.7  | -1   | 0.1111                    |0.1111 * exp(0.881) ≈ 0.2186 |\n",
    "| 0.8  | 1    | 0.0899                    |0.0899 * exp(-0.881) ≈ 0.0451 |\n",
    "| 0.9  | 1    | 0.0899                    |0.0899 * exp(-0.881) ≈ 0.0451 |\n",
    "| 1    | 1    | 0.0899                    |0.0899 * exp(-0.881) ≈ 0.0451 |\n",
    "\n",
    "## **Iteration 3**\n",
    "\n",
    "**Weak Learner for Third Iteration:**\n",
    "Let's say we choose the split at x = 0.6.\n",
    "\n",
    "**Error Calculation:**\n",
    "```\n",
    "Error = (Sum of weights of misclassified samples) / (Sum of all weights)\n",
    "```\n",
    "From the updated weights:\n",
    "- Weights of misclassified samples: 0.0451 (for x = 0.8), 0.0451 (for x = 0.9), 0.0451 (for x = 1)\n",
    "- Sum of weights of misclassified samples: 0.0451 + 0.0451 + 0.0451 = 0.1353\n",
    "\n",
    "Sum of all weights: 1.0\n",
    "\n",
    "```\n",
    "Error = 0.1353 / 1.0 = 0.1353\n",
    "```\n",
    "\n",
    "**Weight of the Weak Learner:**\n",
    "```\n",
    "Weight of weak learner = 0.5 * ln((1 - error) / error)\n",
    "Weight of weak learner = 0.5 * ln((1 - 0.1353) / 0.1353)\n",
    "Weight of weak learner ≈ 0.851\n",
    "```\n",
    "\n",
    "**Updating Weights:**\n",
    "\n",
    "For correctly classified samples:\n",
    "\\[ \\text{New weight}_i = \\text{Old weight}_i \\times e^{-\\text{weight of weak learner}} \\]\n",
    "\n",
    "For misclassified samples:\n",
    "\\[ \\text{New weight}_i = \\text{Old weight}_i \\times e^{\\text{weight of weak learner}} \\]\n",
    "\n",
    "Let's update the weights accordingly and present them in a table.\n",
    "\n",
    "Certainly! Let's merge the \"Before Update\" and \"After Update\" tables to show the weights before and after the third iteration:\n",
    "\n",
    "| x    | y    | Weight (Second Iteration) | Weight (Updated) (Third Iteration) |\n",
    "|------|------|---------------------------|-------------------------------------|\n",
    "| 0.1  | 1    | 0.0451                    | 0.0451 * exp(-0.851) ≈ 0.0231                            |\n",
    "| 0.2  | 1    | 0.0451                    | 0.0451 * exp(-0.851) ≈ 0.0231                              |\n",
    "| 0.3  | 1    | 0.0451                    | 0.0451 * exp(-0.851) ≈ 0.0231                              |\n",
    "| 0.4  | -1   | 0.2186                    | 0.2186 * exp(0.851) ≈ 0.4679                              |\n",
    "| 0.5  | -1   | 0.2186                    | 0.2186 * exp(0.851) ≈ 0.4679                              |\n",
    "| 0.6  | -1   | 0.2186                    | 0.2186 * exp(0.851) ≈ 0.4679                              |\n",
    "| 0.7  | -1   | 0.2186                    | 0.2186 * exp(0.851) ≈ 0.4679                              |\n",
    "| 0.8  | 1    | 0.0451                    | 0.0451 * exp(-0.851) ≈ 0.0231                              |\n",
    "| 0.9  | 1    | 0.0451                    | 0.0451 * exp(-0.851) ≈ 0.0231                              |\n",
    "| 1    | 1    | 0.0451                    | 0.0451 * exp(-0.851) ≈ 0.0231                              |\n",
    "\n",
    "## **Summery**\n",
    "\n",
    "Here's the summary of the AdaBoost iterations:\n",
    "\n",
    "| Round | Split Point | Left Class | Right Class | Alpha   |\n",
    "|-------|-------------|------------|-------------|---------|\n",
    "| 1     | 0.45        | 1          | -1          | 0.4055  |\n",
    "| 2     | 0.25        | 1          | -1          | 0.881   |\n",
    "| 3     | 0.6         | 1          | -1          | 0.851   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "<img src=\"images/adaboost_ex1-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-3.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-5.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-6.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-7.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-8.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-9.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-10.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-11.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-12.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-13.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-14.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-15.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-16.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-17.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-18.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-19.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-20.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-21.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-22.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/adaboost_ex1-23.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting \n",
    "\n",
    "<img src=\"images/gradient-boost-4.webp\">\n",
    "\n",
    "Is a powerful machine learning technique used for both regression and classification problems. It's an ensemble learning method that combines the predictions from multiple individual models to produce a more accurate final prediction. Gradient Boosting builds trees sequentially, where each tree corrects the errors made by the previous one. The process can be summarized as follows:\n",
    "\n",
    "1. **Initialize the model:** Initialize the model with a simple prediction or a constant value.\n",
    "\n",
    "2. **Compute residuals:** Fit a weak model (usually a decision tree) to the data. This model predicts the residuals or errors made by the previous model.\n",
    "\n",
    "3. **Update the model:** Adjust the parameters of the model to minimize the loss function (e.g., mean squared error for regression or cross-entropy loss for classification) between the target values and the predictions made by the model.\n",
    "\n",
    "4. **Repeat:** Repeat steps 2 and 3 until a predefined number of trees have been built, or until a convergence criterion is met.\n",
    "\n",
    "5. **Final prediction:** The final prediction is obtained by summing the predictions of all the weak models.\n",
    "\n",
    "Gradient Boosting differs from other boosting algorithms, such as AdaBoost, by the way it assigns weights to the training data points and how it combines the weak models. In Gradient Boosting, each new model is trained to correct the mistakes of the combined model built up to that point. \n",
    "\n",
    "Some popular implementations of Gradient Boosting include:\n",
    "\n",
    "- **Gradient Boosting Machine (GBM):** The original implementation proposed by Jerome Friedman.\n",
    "- **XGBoost (Extreme Gradient Boosting):** An efficient and scalable implementation of Gradient Boosting, known for its speed and performance.\n",
    "- **LightGBM:** Another efficient and high-performance implementation of Gradient Boosting, developed by Microsoft.\n",
    "- **CatBoost:** A Gradient Boosting library developed by Yandex, designed to handle categorical features efficiently.\n",
    "\n",
    "Gradient Boosting is widely used in various domains, including finance, healthcare, and natural language processing, due to its effectiveness and versatility. However, it can be prone to overfitting, so parameter tuning and regularization are important to prevent this issue.\n",
    "\n",
    "## Example\n",
    "\n",
    "We have below table of sample data with Height, Age and Gender as input variables and weight as the output variable. Target variable is Weight\n",
    "\n",
    "| Height | Age | Gender | Weight |\n",
    "|--------|-----|--------|--------|\n",
    "| 5.4    | 28  | Male   | 88     |\n",
    "| 5.2    | 26  | Female | 76     |\n",
    "| 5      | 28  | Female | 56     |\n",
    "| 5.6    | 25  | Male   | 73     |\n",
    "| 6      | 25  | Male   | 77     |\n",
    "| 4      | 22  | Female | 57     |\n",
    "\n",
    "## Solution\n",
    "\n",
    "If we assume that the average of weights of all the samples as our initial guess then 71.2 (88+76+56+73+77+57/6=71.2) would be our initial root node.\n",
    "\n",
    "Step 1: Building the Initial Tree:\n",
    "Create the root node with the initial guess as the prediction for all samples.\n",
    "\n",
    "| Height | Age | Gender | Weight | Predicted Weight 1 | Pseudo Residuals 1 |\n",
    "|--------|-----|--------|--------|--------------------|--------------------|\n",
    "| 5.4    | 28  | Male   | 88     | 71.2               | 88 - 71.2 = 16.8  |\n",
    "| 5.2    | 26  | Female | 76     | 71.2               | 76 - 71.2 = 4.8   |\n",
    "| 5      | 28  | Female | 56     | 71.2               | 56 - 71.2 = -15.2 |\n",
    "| 5.6    | 25  | Male   | 73     | 71.2               | 73 - 71.2 = 1.8   |\n",
    "| 6      | 25  | Male   | 77     | 71.2               | 77 - 71.2 = 5.8   |\n",
    "| 4      | 22  | Female | 57     | 71.2               | 57 - 71.2 = -14.2 |\n",
    "\n",
    "- Build a new tree to predict these residuals using input variables.\n",
    "- \n",
    "<img src=\"Images/gradient-boost-1.png\" width=\"100%\">\n",
    "\n",
    "- Scale the predictions of each tree by a learning rate (e.g., 0.1).\n",
    "- Combining the trees to make the new prediction. So, we start with initial prediction 71.2 and run the sample data down the new tree and sum them.\n",
    "  \n",
    "| Height | Age | Gender | Weight | Predicted weight 2       |\n",
    "|--------|-----|--------|--------|-----------------------|\n",
    "| 5.4    | 28  | Male   | 88     | 71.2+0.1*16.8=72.9    |       \n",
    "| 5.2    | 26  | Female | 76     | 71.2+0.1*(-5.2)=70.7  |            \n",
    "| 5      | 28  | Female | 56     | 71.2+0.1*(-5.2)=70.7  |            \n",
    "| 5.6    | 25  | Male   | 73     | 71.2+0.1*3.8=71.6     |         \n",
    "| 6      | 25  | Male   | 77     | 71.2+0.1*3.8=71.6     |         \n",
    "| 4      | 22  | Female | 57     | 71.2+0.1*(-14.2)=69.8 |\n",
    "\n",
    "If we observe the new predicted weights, we can see a small improvement in the result compared to the average weight from initial assumption. To further improve the result, we repeat the steps 2 and 3 and build another tree from the new pseudo residuals to predict the weights.\n",
    "\n",
    "| Height | Age | Gender | Weight | Predicted weight 2   | Pseudo Residuals 2 |\n",
    "|--------|-----|--------|--------|----------------------|--------------------\n",
    "| 5.4    | 28  | Male   | 88     | 72.9                 | 88-72.9= 15.1            \n",
    "| 5.2    | 26  | Female | 76     | 70.7                 | 76-70.7=5.3              \n",
    "| 5      | 28  | Female | 56     | 70.7                 | 56-70.7=-14.7           \n",
    "| 5.6    | 25  | Male   | 73     | 71.6                 | 73-71.6= 1.4      \n",
    "| 6      | 25  | Male   | 77     | 71.6                 | 77-71.6 =5.4      \n",
    "| 4      | 22  | Female | 57     | 69.8                 | 57-69.8=-12.8\n",
    "\n",
    "Again build a new tree with the new pseudo residuals.\n",
    "<img src=\"Images/gradient-boost-1.png\" width=\"100%\">\n",
    "\n",
    "Now we combine the new tree with all the previous trees to predict the new weights. So, we start with initial prediction and sum it with scaled result of 1st tree and then sum with scaled result of new tree.\n",
    "\n",
    "| Height | Age | Gender | Weight | Predicted weight 3                   |\n",
    "|--------|-----|--------|--------|--------------------------------------|\n",
    "| 5.4    | 28  | Male   | 88     | 71.2+0.1*16.8+0.1*15.1 = 74.4        |       \n",
    "| 5.2    | 26  | Female | 76     | 71.2+0.1*(-5.2)+0.1*(-4.7) = 70.2    |            \n",
    "| 5      | 28  | Female | 56     | 71.2+0.1*(-5.2)+0.1*(-4.7) = 70.2    |            \n",
    "| 5.6    | 25  | Male   | 73     | 71.2+0.1*3.8+0.1*3.4=71.9            |         \n",
    "| 6      | 25  | Male   | 77     | 71.2+0.1*3.8+0.1*3.4=71.9            |         \n",
    "| 4      | 22  | Female | 57     | 71.2+0.1*(-14.2)+0.1*(-12.8) = 68.5  |\n",
    "\n",
    "From the new predicted weight, we can observe there is further improvement in the result. Again we calculate the pseudo weights and build new tree in the similar way. These steps are repeated several times until the new tree doesn’t decrease the pseudo residual value or till maximum number of trees are built.\n",
    "\n",
    "So the final predicted model would be\n",
    "\n",
    "<img src=\"Images/gradient-boost-3.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c=\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
