{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree:**\n",
    "\n",
    "- **Definition:** Decision Tree is a popular supervised machine learning algorithm used for classification and regression tasks. It organizes data into a tree-like structure where each internal node represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label (in classification) or a numerical value (in regression).\n",
    "\n",
    "- **Example:**\n",
    "  - Suppose we have a dataset of weather conditions and corresponding decisions to play tennis:\n",
    "    - **Outlook**: Sunny, Overcast, Rainy\n",
    "    - **Temperature**: Hot, Mild, Cool\n",
    "    - **Humidity**: High, Normal\n",
    "    - **Wind**: Weak, Strong\n",
    "    - **Decision**: Play, Don't Play\n",
    "\n",
    "| Outlook | Temperature | Humidity | Wind | Decision |\n",
    "|---------|-------------|----------|------|----------|\n",
    "| Sunny   | Hot         | High     | Weak | Don't Play  |\n",
    "| Sunny   | Hot         | High     | Strong | Don't Play  |\n",
    "| Overcast| Hot         | High     | Weak | Play  |\n",
    "| Rainy   | Mild        | High     | Weak | Play  |\n",
    "| Rainy   | Cool        | Normal   | Weak | Play  |\n",
    "| Rainy   | Cool        | Normal   | Strong | Don't Play |\n",
    "| Overcast| Cool        | Normal   | Strong | Play  |\n",
    "| Sunny   | Mild        | High     | Weak | Don't Play |\n",
    "| Sunny   | Cool        | Normal   | Weak | Play  |\n",
    "| Rainy   | Mild        | Normal   | Weak | Play  |\n",
    "| Sunny   | Mild        | Normal   | Strong | Play  |\n",
    "| Overcast| Mild        | High     | Strong | Play  |\n",
    "| Overcast| Hot         | Normal   | Weak | Play  |\n",
    "| Rainy   | Mild        | High     | Strong | Don't Play |\n",
    "\n",
    "- **Mathematical Formulas:**\n",
    "\n",
    "  1. **Entropy (H(S))**:\n",
    "     - Entropy measures the impurity or randomness in a dataset.\n",
    "     - Formula: $ H(S) = -\\sum_{i=1}^{m} p_i \\log_2(p_i) $\n",
    "     - Example: Suppose there are two classes, Yes and No, and the dataset has 9 instances of Yes and 5 instances of No.\n",
    "     - Calculation: $ H(S) = -(\\frac{9}{14} \\log_2(\\frac{9}{14}) + \\frac{5}{14} \\log_2(\\frac{5}{14})) $\n",
    "\n",
    "  2. **Information Gain (IG(A))**:\n",
    "     - Information Gain measures the effectiveness of an attribute in classifying the data.\n",
    "     - Formula: $ IG(S, A) = H(S) - \\sum_{i=1}^{n} \\frac{|S_i|}{|S|} H(S_i) $\n",
    "     - Example: To decide the splitting attribute at the root node, calculate the information gain for each attribute.\n",
    "\n",
    "  3. **Decision Rule**:\n",
    "     - Once the tree is constructed, decision rules at each node determine the traversal path.\n",
    "     - Example: If Outlook is Sunny, go left; if Overcast, go middle; if Rainy, go right.\n",
    "\n",
    "  4. **Pruning (Optional)**:\n",
    "     - Pruning removes branches that do not provide significant improvements.\n",
    "     - Example: Remove nodes with minimal impact on overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "### Taken from https://vtupluse.com/\n",
    "\n",
    "<img src=\"images/ID3-ex1-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-3.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-5.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-6.jpg\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-7.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-8.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-9.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-10.jpg\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-11.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-12.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-13.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex1-14.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "### Taken from https://vtupluse.com/\n",
    "\n",
    "<img src=\"images/ID3-ex2-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-5.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-6.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-7.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-8.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-9.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-10.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-11.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-12.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-13.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-14.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-15.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-16.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-16.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-18.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/ID3-ex2-19.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Limitations of Information Gain**\n",
    "\n",
    "   - One of the limitations of information gain is its bias towards features with a large number of unique values or categories.\n",
    "   - In decision tree algorithms, information gain tends to favor such features, as they can potentially provide more granular splits.\n",
    "\n",
    "## Example\n",
    "\n",
    "Let's consider a simple example to illustrate one of the limitations of information gain: its bias towards features with many values.\n",
    "\n",
    "Suppose we have a dataset representing whether customers purchased a product based on two features: Age (Young, Middle-aged, Senior) and Zip Code (Zip1, Zip2, ..., Zip1000). The target variable is Purchase (Yes/No).\n",
    "\n",
    "| Customer | Age         | Zip Code | Purchase |\n",
    "|----------|-------------|----------|----------|\n",
    "| 1        | Young       | Zip1     | Yes      |\n",
    "| 2        | Young       | Zip2     | Yes      |\n",
    "| 3        | Middle-aged | Zip3     | Yes      |\n",
    "| 4        | Senior      | Zip4     | Yes      |\n",
    "| 5        | Senior      | Zip5     | No       |\n",
    "| 6        | Senior      | Zip6     | Yes      |\n",
    "| 7        | Middle-aged | Zip7     | No       |\n",
    "| 8        | Young       | Zip8     | Yes      |\n",
    "| 9        | Young       | Zip9     | No       |\n",
    "| 10       | Senior      | Zip10    | No       |\n",
    "\n",
    "In this dataset, the Zip Code feature has 1000 unique values (Zip1, Zip2, ..., Zip1000), while the Age feature has only 3 unique values (Young, Middle-aged, Senior).\n",
    "\n",
    "Now, let's calculate the information gain for both features and observe the bias towards the Zip Code feature:\n",
    "\n",
    "### Information Gain for Age:\n",
    "\n",
    "Entropy before split (H_before):\n",
    "$ H_{\\text{before}} = -\\left(\\frac{4}{10} \\log_2 \\frac{4}{10} + \\frac{6}{10} \\log_2 \\frac{6}{10}\\right) $\n",
    "\n",
    "Entropy after split (H_after):\n",
    "$ H_{\\text{after}} = \\frac{4}{10} \\times 0 + \\frac{3}{10} \\times \\text{Entropy}(Young) + \\frac{3}{10} \\times \\text{Entropy}(Middle-aged) + \\frac{3}{10} \\times \\text{Entropy}(Senior) $\n",
    "\n",
    "Information gain (IG):\n",
    "$ IG_{\\text{Age}} = H_{\\text{before}} - H_{\\text{after}} $\n",
    "\n",
    "### Information Gain for Zip Code:\n",
    "\n",
    "Entropy before split (H_before):\n",
    "$ H_{\\text{before}} = -\\left(\\frac{4}{10} \\log_2 \\frac{4}{10} + \\frac{6}{10} \\log_2 \\frac{6}{10}\\right) $\n",
    "\n",
    "Entropy after split (H_after):\n",
    "$ H_{\\text{after}} = \\frac{1}{10} \\times \\text{Entropy}(Zip1) + \\frac{1}{10} \\times \\text{Entropy}(Zip2) + \\ldots + \\frac{1}{10} \\times \\text{Entropy}(Zip1000) $\n",
    "\n",
    "Information gain (IG):\n",
    "$ IG_{\\text{Zip Code}} = H_{\\text{before}} - H_{\\text{after}} $\n",
    "\n",
    "In this example, even if Age is a more informative feature in practice, the Zip Code feature is likely to have a higher information gain simply because it has more unique values. This demonstrates the bias of information gain towards features with many values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Alternative to Entropy: Gini Index**\n",
    "\n",
    "- **Definition**: Gini Index is another impurity measure used in decision tree algorithms, particularly for binary splits. It measures the probability of misclassifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the node.\n",
    "\n",
    "- **Mathematical Formulas**:\n",
    "\n",
    "  1. **Gini Index (Gini(S))**:\n",
    "     - Gini index for a dataset S with m classes:\n",
    "     $ Gini(S) = 1 - \\sum_{i=1}^{m} p_i^2 $\n",
    "     where $ p_i $ is the proportion of instances belonging to class $ i $ in S.\n",
    "     \n",
    "  2. **Gini Index for Splitting (Gini\\_split(A))**:\n",
    "     - Gini index for splitting a dataset S based on attribute A:\n",
    "     $ Gini\\_split(A) = \\sum_{j} \\frac{|S_j|}{|S|} Gini(S_j) $\n",
    "     where $ S_j $ is the subset of S after splitting on attribute A.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "  - Suppose we have a binary classification problem with two classes, Yes and No.\n",
    "  - Consider a node with 10 instances, where 7 instances belong to class Yes and 3 instances belong to class No.\n",
    "  - Calculation of Gini Index:\n",
    "    $ Gini(S) = 1 - \\left(\\left(\\frac{7}{10}\\right)^2 + \\left(\\frac{3}{10}\\right)^2\\right) = 1 - \\left(\\frac{49}{100} + \\frac{9}{100}\\right) = 1 - \\frac{58}{100} = \\frac{42}{100} = 0.42 $\n",
    "\n",
    "- **Binary Splits**:\n",
    "  - Gini Index is particularly useful for binary splits, where an attribute divides the dataset into two subsets.\n",
    "  - At each node, the algorithm calculates the Gini Index for each possible split and chooses the one with the lowest index.\n",
    "\n",
    "- **Comparison with Entropy**:\n",
    "  - Both Gini Index and Entropy are impurity measures used in decision trees.\n",
    "  - Gini Index tends to be faster to compute than entropy because it doesn't involve logarithmic calculations.\n",
    "  - In practice, both measures often lead to similar results, but there may be slight differences in performance depending on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "### Taken from https://vtupluse.com/\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-3.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-5.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-6.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-7.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-8.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-9.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-10.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-11.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-12.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-13.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/DT-Gini-ex1-14.png\" width=\"100%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
