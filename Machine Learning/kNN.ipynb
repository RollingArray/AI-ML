{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "    - KNN is a simple yet powerful supervised machine learning algorithm used for both classification and regression tasks.\n",
    "   - It is based on the assumption that similar data points are close to each other in the feature space.\n",
    "\n",
    "**2. How KNN Works:**\n",
    "   - Given a new data point, KNN finds the K nearest neighbors to that point based on a distance metric (usually Euclidean distance).\n",
    "   - For classification, the class label of the majority of the K neighbors is assigned to the new data point.\n",
    "   - For regression, the average (or another aggregation method) of the target values of the K neighbors is used as the prediction.\n",
    "\n",
    "**3. Key Components of KNN:**\n",
    "   - **K parameter:** Determines the number of neighbors to consider. Choosing an appropriate K is crucial for the performance of the algorithm.\n",
    "   - **Distance metric:** Commonly used metrics include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "   - **Feature scaling:** It's important to scale features to ensure all features contribute equally to the distance calculation.\n",
    "   - **Decision boundary:** KNN's decision boundary is nonlinear and adapts to the distribution of data points.\n",
    "\n",
    "<img src=\"images/knn.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "<img src=\"images/knn_ex1-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex1-2.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "<img src=\"images/knn_ex2-1.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex2-2.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex2-3.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex2-4.png\" width=\"100%\">\n",
    "\n",
    "<img src=\"images/knn_ex2-5.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted K-Nearest Neighbors (KNN)\n",
    "\n",
    "In KNN, all neighbors have equal weight in the decision-making process. However, in real-world datasets, classes may be imbalanced, leading to biased predictions. Weighted K-Nearest Neighbors (KNN) involves assigning weights to each neighbor based on their distance from the query point. These weights are then used to determine the prediction or classification of the query point. Here are the mathematical functions involved in weighted KNN:\n",
    "\n",
    "1. **Distance Calculation:**\n",
    "   - Euclidean Distance:\n",
    "     $ \\text{Euclidean Distance} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $\n",
    "   - Manhattan Distance:\n",
    "     $ \\text{Manhattan Distance} = \\sum_{i=1}^{n} |x_i - y_i| $\n",
    "   - Minkowski Distance:\n",
    "     $ \\text{Minkowski Distance} = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{\\frac{1}{p}} $\n",
    "     where $ p $ is a parameter (typically 2 for Euclidean distance, 1 for Manhattan distance).\n",
    "\n",
    "2. **Kernel Function:**\n",
    "   - The kernel function assigns weights to each neighbor based on their distance from the query point. A commonly used kernel function is the inverse of the squared distance:\n",
    "     $ \\text{Kernel}(d(x_q, x_i)) = \\frac{1}{d(x_q, x_i)^2} $\n",
    "   - Other kernel functions can also be used depending on the application, such as Gaussian kernel, triangular kernel, etc.\n",
    "\n",
    "3. **Weight Calculation:**\n",
    "   - After calculating distances, the weights $ w_i $ for each neighbor are calculated using the kernel function:\n",
    "     $ w_i = \\text{Kernel}(d(x_q, x_i)) $\n",
    "\n",
    "4. **Weighted Voting:**\n",
    "   - The prediction or classification of the query point is determined by weighted voting. The class with the highest total weight among the $ K $ nearest neighbors is assigned to the query point.\n",
    "\n",
    "5. **Prediction:**\n",
    "   - After calculating weights, the prediction for the query point $ x_q $ is determined using weighted voting:\n",
    "     $ \\hat{y}_q = \\arg\\max_{y} \\sum_{i=1}^{K} w_i \\cdot \\mathbb{1}(y_i = y) $\n",
    "     where $ \\hat{y}_q $ is the predicted class for $ x_q $, $ y_i $ is the class of the $ i $-th nearest neighbor, $ w_i $ is the weight assigned to the $ i $-th nearest neighbor, and $ \\mathbb{1}(y_i = y) $ is the indicator function that returns 1 if $ y_i = y $ and 0 otherwise.\n",
    "\n",
    "Weighted K-Nearest Neighbors (KNN) can handle both real-valued and discrete-valued features. The formula for weighted KNN remains the same, but the calculation of distances and weights may differ based on the type of features. Here are the formulas for weighted KNN for both real-valued and discrete-valued features:\n",
    "\n",
    "### For Real-Valued Features:\n",
    "1. **Distance Calculation:**\n",
    "   - Euclidean Distance:\n",
    "     $ \\text{Euclidean Distance} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $\n",
    "   - Other distance metrics like Manhattan distance or Minkowski distance can also be used.\n",
    "\n",
    "2. **Kernel Function:**\n",
    "   - Inverse of Squared Distance:\n",
    "     $ \\text{Kernel}(d(x_q, x_i)) = \\frac{1}{d(x_q, x_i)^2} $\n",
    "   - Other kernel functions can also be used based on the application.\n",
    "\n",
    "3. **Weight Calculation:**\n",
    "   - Weight for each neighbor $ i $ is calculated using the kernel function:\n",
    "     $ w_i = \\text{Kernel}(d(x_q, x_i)) $\n",
    "\n",
    "4. **Weighted Voting:**\n",
    "   - The prediction for the query point $ x_q $ is determined by weighted voting:\n",
    "     $ \\hat{y}_q = \\arg\\max_{y} \\sum_{i=1}^{K} w_i \\cdot \\mathbb{1}(y_i = y) $\n",
    "     where $ \\hat{y}_q $ is the predicted class for $ x_q $, $ y_i $ is the class of the $ i $-th nearest neighbor, $ w_i $ is the weight assigned to the $ i $-th nearest neighbor, and $ \\mathbb{1}(y_i = y) $ is the indicator function.\n",
    "\n",
    "### For Discrete-Valued Features:\n",
    "1. **Distance Calculation:**\n",
    "   - Hamming Distance:\n",
    "     $ \\text{Hamming Distance} = \\sum_{i=1}^{n} \\mathbb{1}(x_i \\neq y_i) $\n",
    "   - Hamming distance counts the number of positions at which the corresponding symbols differ.\n",
    "\n",
    "2. **Kernel Function:**\n",
    "   - The kernel function for discrete-valued features is typically binary:\n",
    "     $ \\text{Kernel}(d(x_q, x_i)) = \\begin{cases} 1 & \\text{if } d(x_q, x_i) = 0 \\\\ 0 & \\text{otherwise} \\end{cases} $\n",
    "     where $ d(x_q, x_i) $ is the Hamming distance between $ x_q $ and $ x_i $.\n",
    "\n",
    "3. **Weight Calculation:**\n",
    "   - Weight for each neighbor $ i $ is determined by the kernel function:\n",
    "     $ w_i = \\text{Kernel}(d(x_q, x_i)) $\n",
    "\n",
    "4. **Weighted Voting:**\n",
    "   - Weighted voting is performed similarly to real-valued features:\n",
    "     $ \\hat{y}_q = \\arg\\max_{y} \\sum_{i=1}^{K} w_i \\cdot \\mathbb{1}(y_i = y) $\n",
    "\n",
    "These are the formulas for weighted KNN for both real-valued and discrete-valued features. The choice of distance metric and kernel function may vary depending on the nature of the data and the specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 (Discrete-Valued)\n",
    "\n",
    "| Sl. No | Height | Weight | Target |\n",
    "|--------|--------|--------|--------|\n",
    "| 1      | 150    | 50     | Medium |\n",
    "| 2      | 155    | 55     | Medium |\n",
    "| 3      | 160    | 60     | Large  |\n",
    "| 4      | 161    | 59     | Large  |\n",
    "| 5      | 158    | 65     | Large  |\n",
    "| 6      | 157    | 54     | ?      |\n",
    "\n",
    "## Solution\n",
    "\n",
    "| Sl. No | Height (X) | Weight (Y) | Target | (x1-x2) | (x1-x2)^2 | (y1-y2) | (y1-y2)^2 | Euclidean Distance | K3 (Nearest point) |\n",
    "|--------|------------|------------|--------|---------|-----------|---------|-----------|--------------------------------------------|-------------------|\n",
    "| 1      | 150        | 50         | Medium | 7       | 49        | 4       | 16        | 8.06                                       |                   |\n",
    "| 2      | 155        | 55         | Medium | 2       | 4         | -1      | 1         | 2.24                                       | 1                 |\n",
    "| 3      | 160        | 60         | Large  | -3      | 9         | -6      | 36        | 6.71                                       | 3                 |\n",
    "| 4      | 161        | 59         | Large  | -4      | 16        | -5      | 25        | 6.40                                       | 2                 |\n",
    "| 5      | 158        | 65         | Large  | -1      | 1         | -11     | 121       | 11.05                                      |                   |\n",
    "| 6      | 157        | 54         | ?      |         |           |         |           |                                            |                   |\n",
    "\n",
    "| 1/d^2 |\n",
    "|-------|\n",
    "| 0.02  |\n",
    "| 0.20  |\n",
    "| 0.02  |\n",
    "| 0.02  |\n",
    "| 0.01  |\n",
    "\n",
    "Predict Target (Medium)\n",
    "= $ 0.20 * δ(Medium, Medium) + 0.02 * δ(Medium, Large) + 0.02 * δ(Medium, Large) $\n",
    "\n",
    "= $ 0.20 * 1 + 0.02 * 0 + 0.02 * 0 $\n",
    "\n",
    "= $ 0.20 $\n",
    "\n",
    "Predict Target (Large)\n",
    "= $ 0.20 * δ(Large, Medium) + 0.02 * δ(Large, Large) + 0.02 * δ(Large, Large) $\n",
    "\n",
    "= $ 0.20 * 0 + 0.02 * 1 + 0.02 * 1 $\n",
    "\n",
    "= $ 0.04 $\n",
    "\n",
    "#### **Hence target category is Medium**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4 (Real-Valued)\n",
    "\n",
    "| Sl. No | Height | Weight | Target |\n",
    "|--------|--------|--------|--------|\n",
    "| 1      | 150    | 50     | 1.5 |\n",
    "| 2      | 155    | 55     | 1.2 |\n",
    "| 3      | 160    | 60     | 1.8  |\n",
    "| 4      | 161    | 59     | 2.1  |\n",
    "| 5      | 158    | 65     | 1.7  |\n",
    "| 6      | 157    | 54     | ?      |\n",
    "\n",
    "## Solution\n",
    "\n",
    "| Sl. No | Height (X) | Weight (Y) | Target | (x1-x2) | (x1-x2)^2 | (y1-y2) | (y1-y2)^2 | Euclidean Distance | K3 (Nearest point) |\n",
    "|--------|------------|------------|--------|---------|-----------|---------|-----------|--------------------------------------------|-------------------|\n",
    "| 1      | 150        | 50         | 1.5 | 7       | 49        | 4       | 16        | 8.06                                       |                   |\n",
    "| 2      | 155        | 55         | 1.2 | 2       | 4         | -1      | 1         | 2.24                                       | 1                 |\n",
    "| 3      | 160        | 60         | 1.8  | -3      | 9         | -6      | 36        | 6.71                                       | 3                 |\n",
    "| 4      | 161        | 59         | 2.1  | -4      | 16        | -5      | 25        | 6.40                                       | 2                 |\n",
    "| 5      | 158        | 65         | 1.7  | -1      | 1         | -11     | 121       | 11.05                                      |                   |\n",
    "| 6      | 157        | 54         | ?      |         |           |         |           |                                            |                   |\n",
    "\n",
    "| 1/d^2 |\n",
    "|-------|\n",
    "| 0.02  |\n",
    "| 0.20  |\n",
    "| 0.02  |\n",
    "| 0.02  |\n",
    "| 0.01  |\n",
    "\n",
    "\n",
    "\n",
    "Predict Target (Medium)\n",
    "= $ \\frac{0.20 * 1.2 + 0.02 * 1.8 + 0.02 * 2.1}{0.20 + 0.02 + 0.02} $\n",
    "\n",
    "= $ 1.325 $\n",
    "\n",
    "#### **Hence target category value is 1.325 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5\n",
    "\n",
    "Based on the information given in the table below, find the customer type of xq=C4 using K- NN. In given example consider all the instances and use locally weighted K-NN algorithm with kernel K(d(xq,xi)) = 1/ d(xq,xi)2.\n",
    "Apply min-max normalization on income to obtain [0,1] range. Consider profession and Region as nominal. Consider :Locality as ordinal variable with ranking order of [Village, Small Town, Suburban, Metropolitan]. Give equal weight to each attribute.\n",
    "\n",
    "Here's the formatted table:\n",
    "\n",
    "| Customer | Income | Profession     | Region   | Locality    | Category |\n",
    "|----------|--------|----------------|----------|-------------|----------|\n",
    "| C0       | 60000  | Doctor         | Hindi    | Village     | L1       |\n",
    "| C1       | 70000  | Doctor         | Bengali  | Village     | L2       |\n",
    "| C2       | 60000  | Carpenter      | Hindi    | Suburban    | L2       |\n",
    "| C3       | 80000  | Doctor         | Bhojpuri | Metropolitan | L2       |\n",
    "| C5       | 80000  | Data Scientist | Hindi    | Small Town  | L1       |\n",
    "| C4       | 50000  | Data Scientist | Hindi    | Small Town  | ?        |\n",
    "\n",
    "## Solution\n",
    "\n",
    "- Use the formula to apply min-max normalization each income value:\n",
    "\n",
    "$ \\text{Normalized Income} = \\frac{\\text{Income} - \\text{Min}(Income)}{\\text{Max}(Income) - \\text{Min}(Income)} $\n",
    "\n",
    "\n",
    "Locality as ordinal variable with ranking order of [Village = 1, Small Town = 2, Suburban = 3, Metropolitan = 4].\n",
    "   - Use the formula to normalize each locality value:\n",
    "$ \\text{Normalized Ranked Locality} = \\frac{\\text{Ranked Locality} - \\text{Min}(Ranked Locality)}{\\text{Max}(Ranked Locality) - \\text{Min}(Ranked Locality)} $\n",
    "\n",
    "\n",
    "| Customer | Income | Normalized Income | Profession     | Region   | Locality    | Ranked Locality | Normalized Scaled Ranked Locality | Category |\n",
    "|----------|--------|-------------------|----------------|----------|-------------|-----------------|------------------------|----------|\n",
    "| C0       | 60000  | 0.33              | Doctor         | Hindi    | Village     | 1               | 0                      | L1       |\n",
    "| C1       | 70000  | 0.67              | Doctor         | Bengali  | Village     | 1               | 0                      | L2       |\n",
    "| C2       | 60000  | 0.33              | Carpenter      | Hindi    | Suburban    | 3               | 0.67                   | L2       |\n",
    "| C3       | 80000  | 1.00              | Doctor         | Bhojpuri | Metropolitan | 4               | 1.00                   | L2       |\n",
    "| C5       | 80000  | 1.00              | Data Scientist | Hindi    | Small Town  | 2               | 0.33                   | L1       |\n",
    "| C4       | 50000  | 0.00              | Data Scientist | Hindi    | Small Town  | 2               | 0.33                   | ?        |\n",
    "\n",
    "\n",
    "To calculate distance between (C4, C2)\n",
    "= distance w.r.t to numerical attributes + distance w.r.t Nominal attributes\n",
    "= Euclidean distance on (Income, Scaled Locality ) + Categorical distance on attributes (Profession, Region)\n",
    "\n",
    "Euclidean distance on (Income, Scaled Locality )\n",
    "= $\\sqrt{(Incomec4 − Incomec2)^2+(Localityc4 − Locailityc2)^2}$\n",
    "\n",
    "= $\\sqrt{(0 − 0.33)^2+(0.33 − 0.67)^2}$ = 0.47\n",
    "\n",
    "Categorical distance on attributes (Profession, Region)\n",
    " = $ \\frac{\\text{No. of categorical features} - \\text{No. of matches between C4 and C2}}{\\text{No. of categorical features}} $\n",
    " \n",
    " = $ \\frac{\\text{2} - \\text{1}}{\\text{2}} $ = 0.5 \n",
    "\n",
    " = 0.47 + 0.5 = 0.97\n",
    "\n",
    "Calculate weight assigned to each attribute.\n",
    "- Kernel function: $K(d(x_q, x_i)) = \\frac{1}{d(x_q, x_i)^2}$.\n",
    "\n",
    "= $\\frac{1}{0.97^2}$ = 1.06\n",
    "\n",
    "| Customer | Distance | Weight | Category |  Rank  |\n",
    "|----------|----------|--------|----------|--------|\n",
    "| C0       | 0.97     | 1.06   | L1       |  4\n",
    "| C1       | 1.75     | 0.33   | L2       |  2\n",
    "| C2       | 0.97     | 1.06   | L2       |  4\n",
    "| C3       | 2.20     | 0.21   | L2       |  1\n",
    "| C5       | 1.00     | 1.00   | L1       |  3\n",
    "\n",
    "Total Weight of L1 = 1.06 + 1.00 = 2.06\n",
    "Total Weight of L2 = 0.33 + 1.06 + 0.21 = 1.60\n",
    "\n",
    "**Inference** : Without weighted KNN, L2 is the majority voted class.\n",
    "But with weighted distances, L1 class instances are more similar to C4 (L1 > L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Locally Weighted Linear Regression**\n",
    "\n",
    "- Locally Weighted Linear Regression (LWLR) is a non-parametric regression technique used for making predictions based on locally weighted least squares.\n",
    "- LWLR is an extension of traditional linear regression that adapts to the local structure of the data.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - LWLR uses a kernel function to assign weights to each data point in the dataset.\n",
    "   - The kernel function determines the weight assigned to each data point based on its distance from the query point.\n",
    "   - Common kernel functions include Gaussian (or Radial Basis Function), triangular, and Epanechnikov kernels.\n",
    "\n",
    "2. **Weighted Least Squares:**\n",
    "   - Once the weights are assigned using the kernel function, LWLR performs a weighted least squares regression.\n",
    "   - Weighted least squares regression fits a linear model by giving higher importance to data points with higher weights.\n",
    "   - Unlike ordinary least squares regression, where all data points have equal importance, weighted least squares accounts for the proximity of data points to the query point.\n",
    "\n",
    "3. **Local Prediction:**\n",
    "   - LWLR predicts the output for a new query point by fitting a linear model to the subset of data points that are closest to the query point.\n",
    "   - The weights assigned by the kernel function ensure that data points closer to the query point have a greater influence on the regression model, while points farther away have less influence.\n",
    "   - This local prediction approach allows LWLR to capture non-linear relationships and adapt to the local structure of the data.\n",
    "\n",
    "4. **Adaptive Nature:**\n",
    "   - One of the main advantages of LWLR is its adaptive nature.\n",
    "   - LWLR adapts to the local structure of the data by assigning higher weights to nearby data points and lower weights to distant ones.\n",
    "   - This adaptability makes LWLR suitable for datasets with complex, non-linear relationships or varying patterns across different regions of the feature space.\n",
    "\n",
    "5. **Hyperparameter Tuning:**\n",
    "   - LWLR involves tuning hyperparameters such as the bandwidth parameter of the kernel function.\n",
    "   - The bandwidth parameter determines the width of the neighborhood around the query point that is considered for local regression.\n",
    "   - Choosing an appropriate bandwidth is crucial as it affects the trade-off between bias and variance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Consider the following training set in 2-dimensional Euclidean space. What is the prize of the product with size 7 if 5-NN is considered? Use the kernel of the form K(d(xq,xi)) = exp( - d(xq,xi)2 / 2b2) where b=1. Use the linear regression of the form y = 3.5 + 0.8 x and the learning rate of 0.2 to apply the gradient descent and update the weights at the end of first iteration\n",
    "\n",
    "| Size | Product Price |\n",
    "|------|---------------|\n",
    "| 10   | 5             |\n",
    "| 5    | 5             |\n",
    "| 2    | 2             |\n",
    "| 3    | 3             |\n",
    "| 5    | 5             |\n",
    "| 15   | 5             |\n",
    "| 30   | 9             |\n",
    "| 20   | 8             |\n",
    "| 6    | 9             |\n",
    "| 2    | 3             |\n",
    "| 10   | 9             |\n",
    "| 13   | 8             |\n",
    "\n",
    "## Solution\n",
    "\n",
    "Let's compute all the kernel function values for each pair of data points. We'll use the provided kernel function:\n",
    "\n",
    "$ K(d(x_q, x_i)) = \\exp\\left(-\\frac{d(x_q, x_i)^2}{2b^2}\\right) $\n",
    "\n",
    "Given $ b = 1 $, let's calculate the kernel values for each pair of $ x_q $ and $ x_i $, where $ x_q = 7 $ and $ x_i $ are the sizes of the nearest neighbors.\n",
    "\n",
    "For $ x_q = 7 $ and the 5 nearest neighbors:\n",
    "\n",
    "$ d(7, 6) = |7 - 6| = 1 $\n",
    "\n",
    "$ d(7, 2) = |7 - 2| = 5 $\n",
    "\n",
    "$ d(7, 2) = |7 - 2| = 5 $\n",
    "\n",
    "$ d(7, 3) = |7 - 3| = 4 $\n",
    "\n",
    "$ d(7, 3) = |7 - 3| = 4 $\n",
    "\n",
    "Now, let's calculate the kernel function values:\n",
    "\n",
    "$ K(1) = \\exp\\left(-\\frac{1^2}{2 \\times 1^2}\\right) = \\exp(-\\frac{1}{2}) \\approx 0.60653 $\n",
    "\n",
    "$ K(5) = \\exp\\left(-\\frac{5^2}{2 \\times 1^2}\\right) = \\exp(-\\frac{25}{2}) \\approx 1.93045 \\times 10^{-6} $\n",
    "\n",
    "$ K(5) = \\exp\\left(-\\frac{5^2}{2 \\times 1^2}\\right) = \\exp(-\\frac{25}{2}) \\approx 1.93045 \\times 10^{-6} $\n",
    "\n",
    "$ K(4) = \\exp\\left(-\\frac{4^2}{2 \\times 1^2}\\right) = \\exp(-\\frac{16}{2}) = \\exp(-8) \\approx 0.00033546 $\n",
    "\n",
    "$ K(4) = \\exp\\left(-\\frac{4^2}{2 \\times 1^2}\\right) = \\exp(-\\frac{16}{2}) = \\exp(-8) \\approx 0.00033546 $\n",
    "\n",
    "Now, we have computed all the kernel function values for each pair of data points. We'll use these values in the subsequent steps for updating the weights. Let's proceed with the next computations.\n",
    "\n",
    "### 2. Compute Errors:\n",
    "We calculate the errors $ f(x_i) - \\hat{f}(x_i) $ for each neighbor using the linear regression equation $ f(x) = 3.5 + 0.8x $:\n",
    "\n",
    "$ \\text{Predicted}_i = 3.5 + 0.8 \\times x_i $\n",
    "\n",
    "$ \\text{Error}_i = y_i - \\text{Predicted}_i $\n",
    "\n",
    "$ \\text{Error}_1 = 9 - (3.5 + 0.8 \\times 6) = 9 - 8.3 = 0.7 $\n",
    "\n",
    "$ \\text{Error}_2 = 5 - (3.5 + 0.8 \\times 2) = 5 - 5.1 = -0.1 $\n",
    "\n",
    "$ \\text{Error}_3 = 5 - (3.5 + 0.8 \\times 2) = 5 - 5.1 = -0.1 $\n",
    "\n",
    "$ \\text{Error}_4 = 3 - (3.5 + 0.8 \\times 3) = 3 - 6.1 = -3.1 $\n",
    "\n",
    "$ \\text{Error}_5 = 3 - (3.5 + 0.8 \\times 3) = 3 - 6.1 = -3.1 $\n",
    "\n",
    "### 3. Compute Feature Values:\n",
    "We'll use the sizes $ x_i $ as feature values. Let's recap the values:\n",
    "\n",
    "$ a(x_1) = 6 $\n",
    "\n",
    "$ a(x_2) = 2 $\n",
    "\n",
    "$ a(x_3) = 2 $\n",
    "\n",
    "$ a(x_4) = 3 $\n",
    "\n",
    "$ a(x_5) = 3 $\n",
    "\n",
    "Now, we have all the necessary values to update the weights using the provided formula. Let's proceed with the calculations.\n",
    "\n",
    "### 4. Update Weights:\n",
    "Using the provided update formula:\n",
    "\n",
    "$ W_j^{new} = W_j^{old} - \\eta \\sum_{i=1}^{5} K(d(x_q, x_i)) (f(x_i) - \\hat{f}(x_i))a(x_i)_j $\n",
    "\n",
    "Let's update each weight $ W_j $:\n",
    "\n",
    "For $ j = 0 $ (bias term):\n",
    "$ W_0^{new} = W_0^{old} - \\eta \\sum_{i=1}^{5} K(d(x_q, x_i)) (f(x_i) - \\hat{f}(x_i)) $\n",
    "\n",
    "$ W_0^{new} = 3.5 - 0.2 \\times (0.60653 \\times 0.7 + 1.93045 \\times 10^{-6} \\times (-0.1) + 1.93045 \\times 10^{-6} \\times (-0.1) + 0.00033546 \\times (-3.1) + 0.00033546 \\times (-3.1)) $\n",
    "\n",
    "$ W_0^{new} = 3.5 - 0.2 \\times (0.424571 + 0.00000193145 - 0.00000193145 - 0.0001039892 - 0.0001039892) $\n",
    "\n",
    "$ W_0^{new} = 3.5 - 0.2 \\times 0.423365 $\n",
    "\n",
    "$ W_0^{new} = 3.5 - 0.084673 $\n",
    "\n",
    "$ W_0^{new} = 3.415327 $\n",
    "\n",
    "For $ j = 1 $ (slope term):\n",
    "$ W_1^{new} = W_1^{old} - \\eta \\sum_{i=1}^{5} K(d(x_q, x_i)) (f(x_i) - \\hat{f}(x_i))a(x_i)_1 $\n",
    "\n",
    "$ W_1^{new} = 0.8 - 0.2 \\times (0.60653 \\times 0.7 \\times 6 + 1.93045 \\times 10^{-6} \\times (-0.1) \\times 2 + 1.93045 \\times 10^{-6} \\times (-0.1) \\times 2 + 0.00033546 \\times (-3.1) \\times 3 + 0.00033546 \\times (-3.1) \\times 3) $\n",
    "\n",
    "$ W_1^{new} = 0.8 - 0.2 \\times (0.60653 \\times 0.7 \\times 6 - 0.000193045 \\times 0.2 - 0.000193045 \\times 0.2 - 0.000311966 \\times 3 - 0.000311966 \\times 3) $\n",
    "\n",
    "$ W_1^{new} = 0.8 - 0.2 \\times (2.133855 - 0.00003861 - 0.00003861 - 0.000935898 - 0.000935898) $\n",
    "\n",
    "$ W_1^{new} = 0.8 - 0.2 \\times 2.131006 $\n",
    "\n",
    "$ W_1^{new} = 0.8 - 0.4262012 $\n",
    "\n",
    "$ W_1^{new} = 0.3737988 $\n",
    "\n",
    "After updating the weights using the kernel regression formula, the new values are approximately:\n",
    "- $ W_0^{new} \\approx 3.415 $\n",
    "- \n",
    "- $ W_1^{new} \\approx 0.374 $"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
